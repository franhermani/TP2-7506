{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\\\n",
    "                             GradientBoostingClassifier, ExtraTreesClassifier,\\\n",
    "                             BaggingClassifier, VotingClassifier)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set de Entrenamiento y Test Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('csv_files/modelo_final.csv')\n",
    "test_final = pd.read_csv('csv_files/test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['rango_edad'] = train['rango_edad'].astype('category')\n",
    "train['sexo'] = train['sexo'].astype('category') \n",
    "train['nivel_estudios'] = train['nivel_estudios'].astype('category')\n",
    "train['esta_estudiando'] = train['esta_estudiando'].astype('category')\n",
    "train['tipo_de_trabajo'] = train['tipo_de_trabajo'].astype('category')\n",
    "train['nivel_laboral'] = train['nivel_laboral'].astype('category')\n",
    "train['nombre_zona'] = train['nombre_zona'].astype('category')\n",
    "train['nombre_area'] = train['nombre_area'].astype('category')\n",
    "train['sepostulo'] = train['sepostulo'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final['rango_edad'] = test_final['rango_edad'].astype('category')\n",
    "test_final['sexo'] = test_final['sexo'].astype('category') \n",
    "test_final['nivel_estudios'] = test_final['nivel_estudios'].astype('category')\n",
    "test_final['esta_estudiando'] = test_final['esta_estudiando'].astype('category')\n",
    "test_final['tipo_de_trabajo'] = test_final['tipo_de_trabajo'].astype('category')\n",
    "test_final['nivel_laboral'] = test_final['nivel_laboral'].astype('category')\n",
    "test_final['nombre_zona'] = test_final['nombre_zona'].astype('category')\n",
    "test_final['nombre_area'] = test_final['nombre_area'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features categóricos:\n",
    "f1 = 'rango_edad'\n",
    "f2 = 'sexo'\n",
    "f3 = 'nivel_estudios'\n",
    "f4 = 'esta_estudiando'\n",
    "f5 = 'tipo_de_trabajo'\n",
    "f6 = 'nivel_laboral'\n",
    "f7 = 'nombre_zona'\n",
    "f8 = 'nombre_area'\n",
    "\n",
    "#features = [f1,f2,f3,f4,f5,f6,f7,f8]\n",
    "features = [f1,f2,f3,f4,f5,f6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(train[features])\n",
    "y = np.array(train['sepostulo'])\n",
    "\n",
    "x_test_final = np.array(test_final[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Definición de algunas variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_aviso_postulante = test_final['id']\n",
    "\n",
    "ntrain = x_train.shape[0] # Cantidad de registros totales del set de entrenamiento\n",
    "\n",
    "ntest = x_test.shape[0] # Cantidad de registros totales del set de pruebas\n",
    "\n",
    "SEED = 0 # Seed\n",
    "\n",
    "NFOLDS = 5 # Cantidad de bloques a particionar para KFold\n",
    "\n",
    "kfold = KFold(ntrain, n_folds=NFOLDS, random_state=SEED) # KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de la clase SklearnPadre\n",
    "Lo que realizamos a continuación es la creación de una clase que posee los métodos comunes de los clasificadores de Sklearn que utilizaremos en el presente trabajo. De esta forma, ahorramos líneas de código y evitamos redundancia a la hora de utilizar los métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El inicializador recibe un clasificador de Sklearn, un seed y los parámetros necesarios del clasificador en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnPadre(object):\n",
    "    def __init__(self, clasif, seed=0, parametros=None):\n",
    "        parametros['random_state'] = seed\n",
    "        self.clasif = clasif(**parametros)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clasif.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clasif.predict(x)\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        return self.clasif.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self, x, y):\n",
    "        print(self.clasif.fit(x,y).feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones OOF (Out Of Fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ensamble utiliza predicciones de los clasificadores base como input para el entrenamiento del modelo de 2do nivel. Sin embargo, no podemos entrenar los modelos base con la totalidad del set de entrenamiento. Esto genera el riesgo de que nuestro modelo de 1er nivel conozca el test final y overfitee cuando utilice las predicciones base.\n",
    "\n",
    "Por ello, utilizamos la siguiente función para tomar distintos bloques del set de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clasif, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros(ntrain)\n",
    "    oof_test = np.zeros(ntest)\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kfold):\n",
    "        x_train_i = x_train[train_index]\n",
    "        y_train_i = y_train[train_index]\n",
    "        x_test_i = x_train[test_index]\n",
    "\n",
    "        clasif.train(x_train_i, y_train_i)\n",
    "\n",
    "        oof_train[test_index] = clasif.predict(x_test_i)\n",
    "        oof_test_skf[i, :] = clasif.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    \n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Primer Nivel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación llevaremos a cabo nuestro primer nivel de clasificación mediante el uso de algunos clasificadores de la librería Sklearn:\n",
    "\n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de los hiper-parámetros necesarios para cada algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "rf_params = {'n_estimators':500, 'max_features':'sqrt', 'max_depth':6, 'min_samples_split':2,\\\n",
    "             'min_samples_leaf':2, 'bootstrap':True, 'oob_score':False, 'warm_start':True}\n",
    "\n",
    "# EXTRA TREES\n",
    "et_params = {'n_estimators':500, 'max_features':'sqrt', 'max_depth':8, 'min_samples_split':2,\\\n",
    "             'min_samples_leaf':2, 'bootstrap':True, 'oob_score':False, 'warm_start':False}\n",
    "\n",
    "# DECISION TREE\n",
    "dt_params = {'criterion':'entropy', 'splitter':'best', 'max_depth':None, 'min_samples_split':2,\\\n",
    "             'min_samples_leaf':2, 'max_features':'sqrt', 'presort':True}\n",
    "\n",
    "# GRADIENT BOOSTING\n",
    "gb_params = {'learning_rate':0.1, 'n_estimators':500, 'max_depth':5, 'min_samples_split':2,\\\n",
    "             'min_samples_leaf':2, 'subsample':1.0, 'max_features':'sqrt', 'warm_start':False}\n",
    "\n",
    "# BAGGING CON LOGISTIC REGRESSION\n",
    "bag_params = {'base_estimator':LogisticRegression(), 'n_estimators':100, 'bootstrap':True,\\\n",
    "              'bootstrap_features':True, 'oob_score':True, 'warm_start':False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los clasificadores según los hiper-parámetros definidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = SklearnPadre(clasif=RandomForestClassifier, seed=SEED, parametros=rf_params)\n",
    "\n",
    "extra_trees = SklearnPadre(clasif=ExtraTreesClassifier, seed=SEED, parametros=et_params)\n",
    "\n",
    "decision_tree = SklearnPadre(clasif=DecisionTreeClassifier, seed=SEED, parametros=dt_params)\n",
    "\n",
    "grad_boost = SklearnPadre(clasif=GradientBoostingClassifier, seed=SEED, parametros=gb_params)\n",
    "\n",
    "bag_log_reg = SklearnPadre(clasif=BaggingClassifier, seed=SEED, parametros=bag_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output de los modelos de primer nivel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación realizamos las predicciones para el set de entrenamiento y el test final con OOF. Estos resultados base serán utilizados como input en el modelo de segundo nivel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_total = time()\n",
    "\n",
    "# Random Forest\n",
    "t0_rf = time()\n",
    "rf_oof_train, rf_oof_test = get_oof(random_forest, x, y, x_test_final)\n",
    "tf_rf = time() - t0_rf\n",
    "print (\"Tiempo de entrenamiento del random forest: %0.5f seconds.\" % tf_rf)\n",
    "\n",
    "# Extra Trees\n",
    "t0_et = time()\n",
    "et_oof_train, et_oof_test = get_oof(extra_trees, x, y, x_test_final)\n",
    "tf_et = time() - t0_et\n",
    "print (\"Tiempo de entrenamiento del extra trees: %0.5f seconds.\" % tf_et)\n",
    "\n",
    "# Decision Tree\n",
    "t0_dt = time()\n",
    "dt_oof_train, dt_oof_test = get_oof(decision_tree, x, y, x_test_final)\n",
    "tf_dt = time() - t0_dt\n",
    "print (\"Tiempo de entrenamiento del decision tree: %0.5f seconds.\" % tf_dt)\n",
    "\n",
    "# Gradient Boosting\n",
    "t0_gb = time()\n",
    "gb_oof_train, gb_oof_test = get_oof(grad_boost, x, y, x_test_final)\n",
    "tf_gb = time() - t0_gb\n",
    "print (\"Tiempo de entrenamiento del gradient boosting: %0.5f seconds.\" % tf_gb)\n",
    "\n",
    "# Bagging con Logistic Regresion\n",
    "t0_bag = time()\n",
    "bag_oof_train, bag_oof_test = get_oof(bag_log_reg, x, y, x_test_final)\n",
    "tf_bag = time() - t0_bag\n",
    "print (\"Tiempo de entrenamiento del bagging con logistic regression: %0.5f seconds.\" % tf_bag)\n",
    "\n",
    "# Tiempo total de ejecución\n",
    "tf_total = time() - t0_total\n",
    "print (\"Tiempo total de entrenamiento del modelo de 1er nivel: %0.5f seconds.\" % tf_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importancia de los features en base a los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf_features = random_forest.feature_importances(x,y)\n",
    "\n",
    "et_features = extra_trees.feature_importances(x,y)\n",
    "\n",
    "dt_features = decision_tree.feature_importances(x,y)\n",
    "\n",
    "gb_features = grad_boost.feature_importances(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACA HAY QUE COPIAR Y PEGAR LA SALIDA DE ARRIBA PERO EN FORMATO ARRAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de un dataframe con la importancia de los features para cada clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "\n",
    "df_features = pd.DataFrame({'Features': features,\\\n",
    "                            'Random Forest': rf_features,\\\n",
    "                            'Extra Trees': et_features,\\\n",
    "                            'Decision Tree': dt_features,\\\n",
    "                            'Gradient Boost': gb_features,\\\n",
    "                            })\n",
    "\n",
    "df_features = df_features[['Features', 'Random Forest', 'Extra Trees', 'Decision Tree', 'Gradient Boost']]\n",
    "\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregamos una columna contenedora del promedio de la importancia de los features de cada fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['promedio'] = df_features.mean(axis=1)\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Segundo Nivel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos las predicciones obtenidas en el modelo de primer nivel como input del siguiente modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, analizamos las predicciones del primer nivel para cada clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_primer_nivel = pd.DataFrame({'Random Forest': rf_oof_train.ravel(),\\\n",
    "                                          'Extra Trees': et_oof_train.ravel(),\\\n",
    "                                          'Decision Tree': dt_oof_train.ravel(),\\\n",
    "                                          'Gradient Boosting': gb_oof_train.ravel(),\\\n",
    "                                          'Bagging con LogReg': bag_oof_train.ravel()\\\n",
    "                                          })\n",
    "predicciones_primer_nivel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenamos las predicciones obtenidas en el primer nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2do_nivel = np.concatenate((et_oof_train, rf_oof_train, dt_oof_train, gb_oof_train, bag_oof_train), axis=1)\n",
    "\n",
    "x_test_final_2do_nivel = np.concatenate((et_oof_test, rf_oof_test, dt_oof_test, gb_oof_test, bag_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicamos XGBoost para el modelo de segundo nivel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicciones con valores binarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Creamos el xgboost\n",
    "xgboost = xgb.XGBClassifier(learning_rate = 0.02,\\\n",
    "                            n_estimators= 2000,\\\n",
    "                            max_depth= 5,\\\n",
    "                            min_child_weight= 2,\\\n",
    "                            gamma=0.9,\\                        \n",
    "                            subsample=0.7,\\\n",
    "                            colsample_bytree=0.7,\\\n",
    "                            objective= 'binary:logistic',\\\n",
    "                            nthread= -1,\\\n",
    "                            scale_pos_weight=1)\n",
    "\n",
    "# Lo entrenamos con el set de entrenamiento obtenido en el modelo de 1er nivel\n",
    "xgboost.fit(x_2do_nivel, y)\n",
    "\n",
    "# Realizamos las predicciones con el set definitivo\n",
    "pred_final = xgboost.predict(x_test_final_2do_nivel)\n",
    "\n",
    "tf = time() - t0\n",
    "print (\"Tiempo de ejecución: %0.5f seconds.\" % tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicciones con probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Creamos el xgboost\n",
    "xgboost = xgb.XGBClassifier(learning_rate = 0.02,\\\n",
    "                            n_estimators= 2000,\\\n",
    "                            max_depth= 5,\\\n",
    "                            min_child_weight= 2,\\\n",
    "                            gamma=0.9,\\                        \n",
    "                            subsample=0.7,\\\n",
    "                            colsample_bytree=0.7,\\\n",
    "                            objective= 'binary:logistic',\\\n",
    "                            nthread= -1,\\\n",
    "                            scale_pos_weight=1)\n",
    "\n",
    "# Lo entrenamos con el set de entrenamiento obtenido en el modelo de 1er nivel\n",
    "xgboost.fit(x_2do_nivel, y)\n",
    "\n",
    "# Realizamos las predicciones con el set definitivo\n",
    "pred_final_proba = xgboost.predict_proba(x_test_final_2do_nivel)\n",
    "\n",
    "# Nos quedamos con la columna correspondiente de probabilidades\n",
    "df_predicciones = pd.DataFrame(pred_final_proba)\n",
    "pred_final_proba = np.array(df_predicciones[1])\n",
    "\n",
    "tf = time() - t0\n",
    "print (\"Tiempo de ejecución: %0.5f seconds.\" % tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit con valores binarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame({'id':id_aviso_postulante, 'sepostulo':pred_final})\n",
    "submit.to_csv('submits/submit_ensamble.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['sepostulo'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit con probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_proba = pd.DataFrame({'id':id_aviso_postulante, 'sepostulo':pred_final_proba})\n",
    "submit_proba.to_csv('submits/submit_ensamble_proba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = submit_proba['sepostulo'] < 0.5\n",
    "si = submit_proba['sepostulo'] >= 0.5\n",
    "\n",
    "cant_no = submit_proba.loc[(no)].count()\n",
    "cant_si = submit_proba.loc[(si)].count()\n",
    "\n",
    "print(\"0   \", cant_no[1])\n",
    "print(\"1   \", cant_si[1])\n",
    "print(\"Name: sepostulo, dtype: int64\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
